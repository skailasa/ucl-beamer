@article{Sundar2013,
abstract = {In this paper, we present HykSort, an optimized comparison sort for distributed memory architectures that attains more than 2× improvement over bitonic sort and samplesort. The algorithm is based on the hypercube quicksort, but instead of a binary recursion, we perform a k-way recursion in which the pivots are selected accurately with an iterative parallel select algorithm. The single-node sort is performed using a vectorized and multithreaded merge sort. The advantages of HykSort are lower communication costs, better load balancing, and avoidance of O(p)-collective communication primitives. We also present a staged communication samplesort, which is more robust than the original samplesort for large core counts. We conduct an experimental study in which we compare hypercube sort, bitonic sort, the original samplesort, the staged samplesort, and HykSort. We report weak and strong scaling results and study the effect of the grain size. It turns out that no single algorithm performs best and a hybridization strategy is necessary. As a highlight of our study, on our largest experiment on 262,144 AMD cores of the CRAY XK7 "Titan" platform at the Oak Ridge National Laboratory we sorted 8 trillion 32-bit integer keys in 37 seconds achieving 0.9TB/s effective throughput. {\textcopyright} 2013 ACM.},
author = {Sundar, Hari and Malhotra, Dhairya and Biros, George},
doi = {10.1145/2464996.2465442},
file = {:home/sri/Documents/Mendeley/HykSort_ a new variant of hypercube quicksort on distributed memory architectures.pdf:pdf},
isbn = {9781450321303},
journal = {Proceedings of the International Conference on Supercomputing},
keywords = {bitonic sort,distributed-memory parallelism,hypercube,parallel algorithms,quicksort,samplesort,shared-memory parallelism,sorting},
pages = {293--302},
title = {{HykSort: A new variant of hypercube quicksort on distributed memory architectures}},
year = {2013}
}
@article{Suh2020,
abstract = {The p4est library implements octree-based adaptive mesh refinement (AMR) and has demonstrated parallel scalability beyond 100,000 MPI processes in previous weak scaling studies. This work focuses on the strong scalability of mesh adaptivity in p4est, where the communication pattern of the existing 2:1-balance is a latency bottleneck. The sorting-based algorithm of Malhotra and Biros has balanced communication, but synchronizes all processes. We propose an algorithm that combines sorting and neighbor-to-neighbor exchange to minimize the number of processes each process synchronizes with.We measure the performance of these algorithms on several test problems on Stampede2 at TACC. Both the parallel-sorting and minimally-synchronous algorithms significantly outperform the existing algorithm and have nearly identical performance out to 1,024 Xeon Phi KNL nodes, meaning the asymptotic advantage of the minimally-synchronous algorithm does not translate to improved performance at this scale. We conclude by showing that global metadata communication will limit future strong scaling.},
author = {Suh, Hansol and Isaac, Tobin},
doi = {10.1109/SC41405.2020.00027},
file = {:home/sri/Documents/Mendeley/Evaluation_of_a_Minimally_Synchronous_Algorithm_for_21_Octree_Balance.pdf:pdf},
isbn = {9781728199986},
issn = {21674337},
journal = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
number = {Section VI},
title = {{Evaluation of a minimally synchronous algorithm for 2:1 octree balance}},
volume = {2020-Novem},
year = {2020}
}
@article{Isaac,
abstract = {The logical structure of a forest of octrees can be used to create scalable algorithms for parallel adaptive mesh refinement (AMR), which has recently been demonstrated for several petascale applications. Among various frequently used octree-based mesh operations, including refinement, coarsen-ing, partitioning, and enumerating nodes, ensuring a 2:1 size balance between neighboring elements has historically been the most expensive in terms of CPU time and communication volume. The 2:1 balance operation is thus a primary target to optimize. One important component of a parallel balance algorithm is the ability to determine whether any two given octants have a consistent distance/size relation. Based on new logical concepts we propose fast algorithms for making this decision for all types of 2:1 balance conditions in 2D and 3D. Since we are able to achieve this without constructing any parent nodes in the tree that would otherwise need to be sorted and communicated, we can significantly reduce the required memory and communication volume. In addition, we propose a lightweight collective algorithm for reversing the asymmetric communication pattern induced by non-local octant interactions. We have implemented our improvements as part of the open-source "p4est" software. Benchmarking this code with both synthetic and simulation-driven adapted meshes we are able to demonstrate much reduced runtime and excellent weak and strong scalability. On our largest benchmark problem with 5.13 × 10 11 octants the new 2:1 balance algorithm executes in less than 8 seconds on 112,128 CPU cores of the Jaguar Cray XT5 supercomputer.},
author = {Isaac, Tobin and Burstedde, Carsten and Ghattas, Omar},
file = {:home/sri/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Isaac, Burstedde, Ghattas - Unknown - Low-Cost Parallel Algorithms for 21 Octree Balance.pdf:pdf},
keywords = {Adaptive mesh refinement,High performance computing,Octrees,Parallel algo-rithms,Scientific computing},
title = {{Low-Cost Parallel Algorithms for 2:1 Octree Balance}}
}
@article{Malhotra2016,
abstract = {The solution of a constant-coefficient elliptic Partial Differential Equation (PDE) can be computed using an integral transform: A convolution with the fundamental solution of the PDE, also known as a volume potential. We present a Fast Multipole Method (FMM) for computing volume potentials and use them to construct spatially adaptive solvers for the Poisson, Stokes, and low-frequency Helmholtz problems. Conventional N-body methods apply to discrete particle interactions. With volume potentials, one replaces the sums with volume integrals. Particle N-body methods can be used to accelerate such integrals. but it is more efficient to develop a special FMM. In this article, we discuss the efficient implementation of such an FMM. We use high-order piecewise Chebyshev polynomials and an octree data structure to represent the input and output fields and enable spectrally accurate approximation of the near-field and the Kernel Independent FMM (KIFMM) for the far-field approximation. For distributed-memory parallelism, we use space-filling curves, locally essential trees, and a hypercube-like communication scheme developed previously in our group. We present new near and far interaction traversals that optimize cache usage. Also, unlike particle N-body codes, we need a 2:1 balanced tree to allow for precomputations. We present a fast scheme for 2:1 balancing. Finally, we use vectorization, including the AVX instruction set on the Intel Sandy Bridge architecture to get better than 50% of peak floating-point performance. We use task parallelism to employ the Xeon Phi on the Stampede platform at the Texas Advanced Computing Center (TACC). We achieve about 600GFLOP/s of double-precision performance on a single node. Our largest run on Stampede took 3.5s on 16K cores for a problem with 18E+9 unknowns for a highly nonuniform particle distribution (corresponding to an effective resolution exceeding 3E+23 unknowns since we used 23 levels in our octree). CCS Concepts: r Mathematics of computing → Mathematical software; Additional Key Words and Phrases: FMM, N-body problems, potential theory ACM Reference Format: Dhairya Malhotra and George Biros. 2016. Algorithm 967: A distributed-memory fast multipole method for volume potentials.},
author = {Malhotra, Dhairya and Biros, George},
doi = {10.1145/2898349},
file = {:home/sri/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Malhotra, Biros - 2016 - Algorithm 967 A Distributed-Memory Fast Multipole Method for Volume Potentials.pdf:pdf},
journal = {ACM Trans. Math. Softw},
title = {{Algorithm 967: A Distributed-Memory Fast Multipole Method for Volume Potentials}},
url = {http://dx.doi.org/10.1145/2898349},
volume = {43},
year = {2016}
}
@article{Lashuk2009,
abstract = {We present new scalable algorithms and a new implementation of our kernel-independent fast multipole method (Ying et al. ACM/IEEE SC '03), in which we employ both distributed memory parallelism (via MPI) and shared memory/streaming parallelism (via GPU acceleration) to rapidly evaluate two-body non-oscillatory potentials. On traditional CPU-only systems, our implementation scales well up to 30 billion unknowns on 65K cores (AMD/CRAY-based Kraken system at NSF/NICS) for highly non-uniform point distributions. On GPU-enabled systems, we achieve 30× speedup for problems of up to 256 million points on 256 GPUs (Lincoln at NSF/NCSA) over a comparable CPU-only based implementations. We achieve scalability to such extreme core counts by adopting a new approach to scalable MPI-based tree construction and partitioning, and a new reduction algorithm for the evaluation phase. For the sub-components of the evaluation phase (the direct-and approximate-interactions, the target evaluation, and the source-to-multipole translations), we use NVIDIA's CUDA framework for GPU acceleration to achieve excellent performance. To do so requires carefully constructed data structure transformations, which we describe in the paper and whose cost we show is minor. Taken together, these components show promise for ultrascalable FMM in the petascale era and beyond.},
author = {Lashuk, Ilya and Chandramowlishwaran, Aparna and Langston, Harper and Nguyen, Tuan-Anh and Sampath, Rahul and Shringarpure, Aashay and Vuduc, Richard and Ying, Lexing and Zorin, Denis and Biros, George},
file = {:home/sri/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lashuk et al. - 2009 - A massively parallel adaptive fast-multipole method on heterogeneous architectures.pdf:pdf},
isbn = {9781605587448},
title = {{A massively parallel adaptive fast-multipole method on heterogeneous architectures}},
year = {2009}
}
@article{Sundar2007,
abstract = {In this article, we propose new parallel algorithms for the construction and 2:1 balance refinement of large linear octrees on distributed memory machines. Such octrees are used in many problems in computational science and engineering, e.g., object representation, image analysis, unstructured meshing, finite elements, adaptive mesh refinement, and N-body simulations. Fixed-size scalability and isogranular analysis of the algorithms using an MPI-based parallel implementation was performed on a variety of input data and demonstrated good scalability for different processor counts (1 to 1024 processors) on the Pittsburgh Supercomputing Center's TCS-1 AlphaServer. The results are consistent for different data distributions. Octrees with over a billion octants were constructed and balanced in less than a minute on 1024 processors. Like other existing algorithms for constructing and balancing octrees, our algorithms have O(N log N) work and O(N) storage complexity. Under reasonable assumptions on the distribution of octants and the work per octant, the parallel time complexity is O(N/nP log(N/nP) + n P]og nP), where N is the size of the final linear octree and nP is the number of processors. {\textcopyright} 2008 Society for Industrial and Applied Mathematics.},
author = {Sundar, Hari and Sampath, Rahul S. and Biros, George},
doi = {10.1137/070681727},
file = {:home/sri/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/SUNDAR, SAMPATH,, BIROS - 2008 - BOTTOM-UP CONSTRUCTION AND 21 BALANCE REFINEMENT OF LINEAR OCTREES IN PARALLEL.pdf:pdf},
issn = {10648275},
journal = {SIAM Journal on Scientific Computing},
keywords = {Balance refinement,Large scale parallel computing,Linear octrees,Morton encoding,Space filling curves},
mendeley-groups = {Doctorate},
number = {5},
pages = {2675--2708},
title = {{Bottom-up construction and 2:1 balance refinement of linear octrees in parallel}},
volume = {30},
year = {2007}
}
