\section{Fast Python}
\subsection{Is Numba Up To It?}
\subsection{Language Bottlenecks for Speed}

\begin{frame}
\frametitle{Is Numba Up To It?}

Problem: Laplace Kernel applied to interactions between 20,000 particles to find Gram Matrix
\begin{itemize}
    \item Numba: $\sim$ 296ms
    \item Rust: $\sim$ 155ms
\end{itemize}

So at first glance, yes - Numba appears to be useful.
\end{frame}

\begin{frame}
\frametitle{Is Numba Up To It?}
    Important caveats for benchmark,
    \begin{itemize}
        \item Numba targeting all CPUs via prange operator
        \item Data is already arranged `nicely'
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Language Bottlenecks For Speed}
    \begin{itemize}
        \item Native Python objects are slow for our purposes, with important implications for data access
        \item Python's numeric libraries (Numpy/Numba) etc haven't focussed on distributing data as much as computation. There are moves in this direction (Legate)
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Language Bottlenecks For Speed}
    Problems faced by PyExaFMM:
    \begin{itemize}
        \item How do we store tree efficiently if we want to avoid native PyObjects?
        \item How do we access precomputed operators and coordinate data without dictionaries, or access to pointers?
        \item How can we accelerate computations on the tree structure?
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Language Bottlenecks For Speed}
    Laplace Problem Benchmark: 1e6 randomly distributed points, max 100 particles per node, order 5 multipole expansions and order 6 local expansions, with M2L compression rank of 1.

    \begin{itemize}
        \item FMM takes 15s on my i7 processor.
        \item Precomputations of the tree and operators, accelerated with GPU, takes 9s, with my NVidia Quadro RTX-3000 GPU.
    \end{itemize}

\end{frame}